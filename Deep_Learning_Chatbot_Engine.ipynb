{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkarimwahba/colab/blob/main/Deep_Learning_Chatbot_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2MpLBvNUekd"
      },
      "source": [
        "*A quick note: You can run all of the code blocks below by hovering your mouse over the brackets at the top left of each block and clicking the play button that appears. In order to run a block of code, you have to have already run the subsequent blocks (In other words you have to run the code blocks in order)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrcAGYQmq_dL"
      },
      "source": [
        "# **Creating a Chatbot With NLP and Deep Learning**\n",
        "In this project, we will be creating a chatbot using natural language processing and deep learning. The chatbot will be able to converse about any topics that we specify and can use these strategies to recognize a diversity of conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnpxeiRz9mlP"
      },
      "source": [
        "# **The Dataset**\n",
        "Here is the dataset we will be using: [Chatbot Intents Dataset](https://drive.google.com/file/d/1JSnm_xk_suPk7yaTEKqbGUAp5TtMkNVv/view?usp=sharing)\n",
        "\n",
        "***If you want to follow along and run the code below, click on the dataset above and hit the download button in the top right.***\n",
        "\n",
        "This dataset holds all of the general topics that our chatbot will be able to talk about. Note that the dataset is written in json, not Python. It looks almost exactly the same as Python except for a few differences so it should still make sense.\n",
        "\n",
        "We will call any topic that our chatbot can talk about and \"intent.\" The dataset is composed of all of these intents and our chatbot will be able to talk about every intent in the dataset.\n",
        "\n",
        "Each intent follows the same basic formatting for the code. Every unique intent has a specific 'tag' to denote the intent. Additionally, each has a list of the possible things a user could say that would fall under the category of the intent which we will call 'patterns.' Each intent also has a list of possible responses for the chatbot to return which we will call 'responses.' Finally, some of the intents have a 'context_set' or 'context_filter' (although not all necessarily need to have these features). These will allow our robot to have a short term memory and engage in contextual conversation. We will talk more about this later.\n",
        "\n",
        "A single intent in our dataset would look like the following:\n",
        "```\n",
        "{\"tag\": \"greeting\",\n",
        "\"patterns\": [\"hello\", \"hi\", \"greetings\"],\n",
        "\"responses\": [\"hi human how are you\", \"hello, how are you\", \"how are you doing\"]\n",
        "\"context_set\": \"how_are_you\"}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3NLHcDnP8y"
      },
      "source": [
        "**Upload the Dataset to Python**\n",
        "\n",
        "1.   Run the below code and click the \"Choose Files\" button that appears\n",
        "2.   Select the \"intents.json\" file from your file explorer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bruHsK9VaT"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAG1ziPX1ZVG"
      },
      "source": [
        "3. Wait until the dataset is 100% done loading\n",
        "4. Now run the code below and the dataset is ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuLUY7HJ1gx2"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('train-v2.0.json') as file:\n",
        "    intents = json.load(file, strict = False) # We don't read the file in strictly so we can use Python regular expressions like '/n' (this is very minor)\n",
        "intents = intents['intents'] # Get all of the individual intents from our dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train-v2.0.json') as file:\n",
        "    data = json.load(file, strict=False)\n",
        "intents = data['data']\n",
        "# Print the keys to understand the structure\n",
        "print(\"Keys in the JSON file:\", data.keys())\n"
      ],
      "metadata": {
        "id": "_9S2gR2wLZUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34bDklNq2h_a"
      },
      "source": [
        "5. Run the code below to see what our dataset looks like right now. (The code just prints out the dataset nicely so that it is readable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gvDT7A2nwV"
      },
      "source": [
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-5gToXc7is7"
      },
      "source": [
        "# **Import the Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5v1HbXE7qhF"
      },
      "source": [
        "Here, we install & import all of the remaining libraries that we will need to use (we already imported json to read in our dataset, and we will be importing nltk for natural language processing in the next step). You will see where we use each of these libraries later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqu4JuMAVSH"
      },
      "source": [
        "#tflearn requires us to install an earlier version of tensorflow\n",
        "!pip install tensorflow==2.8\n",
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPwKP4Faj26X"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tflearn\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p1N7bBp7_-5"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "Our final goal with this project is to train our chatbot based on the intents dataset to recognize which intent a users statement might fall under. The chatbot would then respond accordingly with a response from the given intent.\n",
        "\n",
        "In order to accomplish this, we first need to process our intents dataset using **Natural Language Processing** or NLP. This allows us to simplify the dataset to make it more easy to understand for our chatbot. The first thing we will need to do is download the required libraries for NLP. We will be using a library called the Natural Language Toolkit or NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNPimb9-kL_I"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am6_zQQT9t6v"
      },
      "source": [
        "Now we can get into the specifics of the NLP. This is a long block of code which I cannot break up, so I will write comments to explain the code. Look out for any green text (these are the comments)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvIb70C8kYtM"
      },
      "source": [
        "#This variable will allow us to save a lot of time later if we just want to talk to our chatbot without retraining it.\n",
        "#In colab, you don't need this because you can run each code block seperately, but if you are on offline Python, its nice to have this feature\n",
        "#So I decided to include it.\n",
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = [] #This will be a list of all the words used in any of the 'patterns' in each intent\n",
        "    all_tags = [] #This will be a list of all the 'tag's associated with the intents\n",
        "    intent_patterns = [] #This will be a list containing all of the 'patterns' for each intent where each individual pattern is grouped together\n",
        "    intent_tags = [] #This will be a list correlated with 'intent_patterns' where every pattern in 'intent_patterns' is correlated with its respective\n",
        "                     #Tag in this list\n",
        "\n",
        "    #Here we fill in all of the lists above. Note that we tokenize the words in each pattern which means we split each pattern into individual words\n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "\n",
        "        all_tags.append(intent['tag'])\n",
        "\n",
        "    #Here we stem the words in all_words. This means that we reduce every word down to its root form or stem. This will prevent our chatbot from confusin\n",
        "    #Very similar words with eachother. For example, the chatbot might normally confuse the words 'running' and 'run' because they appear different even\n",
        "    #Though they effectively mean the same thing. Stemming will reduce both of these words down to their root form which would be 'run' so the chatbot will\n",
        "    #No longer be confused\n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "\n",
        "    all_tags = sorted(all_tags)\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "\n",
        "    #Here we are creating our training set input and output values for our deep learning algorithm\n",
        "    #We will do this by iterating through our intents and turning each one into a bag of words, or a vector that indicates which words are in each pattern.\n",
        "    #These bags of words will be the x values and the y values will be the intent that each bag of words is associated with.\n",
        "    #The machine learning will train on this data and will be able to determine which bag of words its corresponding intent.\n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "\n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "\n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "\n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "\n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "\n",
        "    #Here is the data we will be using to train our neural network later\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    #Here we just save our training data so we don't need to process it again if we just want to run our chatbot\n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download nltk data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create a stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the new dataset\n",
        "with open('train-v2.0.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = []\n",
        "answers = []\n",
        "intent_tags = []\n",
        "\n",
        "for item in data['data']:\n",
        "    for paragraph in item['paragraphs']:\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "\n",
        "            # Check if the answers list is not empty before accessing its first element\n",
        "            if qa['answers']:\n",
        "                answer_text = qa['answers'][0]['text']\n",
        "            else:\n",
        "                answer_text = \"No answer available\"  # Provide a default value or handle as needed\n",
        "\n",
        "            questions.append(question)\n",
        "            answers.append(answer_text)\n",
        "            intent_tags.append(item['title'])\n",
        "\n",
        "# Preprocess the questions\n",
        "all_words = []\n",
        "x_train = []\n",
        "y_train = []\n",
        "y_empty = list(set(intent_tags))\n",
        "\n",
        "for index, question in enumerate(questions):\n",
        "    words = nltk.word_tokenize(question)\n",
        "    words = [stemmer.stem(word.lower()) for word in words]\n",
        "\n",
        "    all_words.extend(words)\n",
        "\n",
        "    bag_of_words = [1 if word in words else 0 for word in all_words]\n",
        "\n",
        "    one_hot_encode_y = [1 if tag == intent_tags[index] else 0 for tag in y_empty]\n",
        "\n",
        "    x_train.append(bag_of_words)\n",
        "    y_train.append(one_hot_encode_y)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Save the training data\n",
        "with open('training_data_new.pickle', 'wb') as f:\n",
        "    pickle.dump((all_words, y_empty, x_train, y_train), f)\n"
      ],
      "metadata": {
        "id": "Sqe7YhUMOmmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5seVSXjkin"
      },
      "source": [
        "One of the most important concepts here is the **bag of words** that we are turning each individual pattern into. A bag of words is basically a sentence represented in a vector format. This numerical format is more easily understandable for our neural network. Here is an example of a bag of words to help you understand this important concept.\n",
        "\n",
        "Let's say we want to represent the following sentences as bags of words:\n",
        "1. The boy jumped high\n",
        "2. The dog jumped up\n",
        "\n",
        "What we will do is first associate every unique word in those sentences with an entry in a vector. Our total vector will be 6 entries long (since their are 6 unique words in these sentences), and the first entry will be correlated with the word 'the,' the second with 'boy,' the third with 'jumped,' the fourth with 'high,' the fifth with 'dog,' and the sixth with 'up.' Thus we can represent each of these sentences with a vector where a 1 represents the sentences containing the word corresponding to the entry in the vector.\n",
        "\n",
        "In this case, our vectors would look like this:\n",
        "1. [1, 1, 1, 1, 0, 0] since the sentence contains the first four words in the vector 'the,' 'boy,' 'jumped,' 'high'\n",
        "2. [1, 0, 1, 0, 1, 1] since the sentence contains the following words which are associated with the first, third, fifth, and sixth spots of the vector: 'the,' 'dog,' 'jumped,' 'high.'\n",
        "\n",
        "Notice that with these vectors, we are able to completely represent the meaning of the sentences through numbers rather than words. What we are doing in this code is essentially repeating this same process except for every pattern for every intent of our chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_iFyoqBn8w"
      },
      "source": [
        "# **Using Deep Learning**\n",
        "Now that we have completed the NLP stage, we can move onto the actual deep learning where we will train our neural networks. We will be creating a neural that recognizes the correlations between the bags of words we created for each pattern and the corresponding intent. Below, we create a neural network with an input layer that takes in these bags of words, two hidden layers, and an output layer that gives a probability for the bag of words being correlated with each intent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckSETlJPluc1"
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "\n",
        "#Create the neural network layers\n",
        "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "#Here we use the softmax activation function so the output of our neural network is a probability. We will make use of this later\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivgU7eaFmMtK"
      },
      "source": [
        "Now that we have created our neural network, we can train it and save our model for later. Run the code below to train the neural network using the bags of words we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUtrneXlvWf"
      },
      "source": [
        "#Again, this doesn't do much in colab, but on offline Python, this helps to save time.\n",
        "if retrain_model:\n",
        "    #Here we train the neural network with the training data we created in the NLP stage\n",
        "    model.fit(x_train, y_train, n_epoch = 500, batch_size = 8, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcT1bUMZBs7g"
      },
      "source": [
        "# **Creating the Chatbot**\n",
        "Now that we have our neural network model trained, all we have to do is make the interface where the user is actually able to chat with our chatbot. We want to convert the users input to a bag of words and then feed this bag of words into our neural network. From there, we will check which intent the users bag of words is most highly correlated with and then we will get a response from that intent.\n",
        "\n",
        "Below, we create the first step of our process. The first thing we need to do is convert the users input to a bag of words. That's exactly what the function below does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQtvD_EkRrD_"
      },
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    #Initialize the bag of words by creating an empty slot for every word in the vector\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "\n",
        "    #First we split up the input into individual words and stem them so they match the same format as in our vector\n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "\n",
        "    #Now we create the bag of words by filling in a 1 for the words that the user used\n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "\n",
        "    #And return the bag of words\n",
        "    return np.array(bag_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iamOiVqbn7kJ"
      },
      "source": [
        "Now we can create the actual chat function that the user can interact with. This function will get the users input, call the bag_of_words function to turn it into a bag of words, and then pass the bag of words into the neural network to get a prediction. Finally, it will print out the chatbots response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKsYmRq8lxjU"
      },
      "source": [
        "def chat():\n",
        "    #Starting message\n",
        "    print(\"Enter a message to talk to the bot [type quit to exit].\")\n",
        "\n",
        "    #Reset the context state since there is no context at the beginning of the conversation\n",
        "    context_state = None\n",
        "\n",
        "    #This is what the bot will say if it doesn't understand what the user is saying\n",
        "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
        "                         'You confuse me human. Lets talk about something else.',\n",
        "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
        "                         'I dont understand that! Try rephrasing or saying something else.']\n",
        "\n",
        "    #This chat loop will go on forever until the user types quit\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        #Convert chat to bag of words\n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        #Pass bag of words into our neural network\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        #Get the intent that the bag of words is most highly correlated with\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "\n",
        "        #If the neural network is fairly certain that it has chosen the right intent (and isnt randomly guessing)\n",
        "        #In this case, we will only get a response if the neural network is more than 80% certain\n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                #Get the intent that is predicted\n",
        "                if intent['tag'] == response_tag:\n",
        "                    #Check if this response is associated with a specific context\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        #Get all of the possible responses from this intent\n",
        "                        possible_responses = intent['responses']\n",
        "                        #If this intent is associated with a context set, then set the context state\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        #Select a random message from the intent responses\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        #Print a did not understand message\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            #Print a did not understand message\n",
        "            print(random.choice(default_responses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J3BbINagnn-"
      },
      "source": [
        "Now our chatbot is complete and we can actually talk to it! Run the code below to talk to the chatbot. Notice how any of the intents that are specified in our dataset will be recognized by the chatbot. You don't need to word your phrases exactly how they are worded in the dataset because of the NLP we used to simplify language combined with our deep learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu9liIl4l16e"
      },
      "source": [
        "chat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWdpms0EhOQd"
      },
      "source": [
        "As you can see, our chatbot works pretty well with the intents in our dataset. This is only the beginning for our chatbot though. In fact, you can make your own json datasets with your own chatbot intents using the format specified earlier and you can run it with this chatbot engine.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGLsfQ2ZiF9Y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMzhUHzShs77"
      },
      "source": [
        "# **WAIT... Don't click away yet!**\n",
        "If you want to learn more about natural language processing and how it can be used, check out my article here:\n",
        "* Medium Article: https://medium.com/@mr.adam.maj/machines-key-to-understanding-humans-how-i-used-natural-language-processing-to-analyze-human-9745d04e534b?sk=308d99c0f21a59b17b1734291b889654\n",
        "\n",
        "If you enjoyed reading about this project and are curious to see another one, check out my previous project where I experimented with reinforcement learning:\n",
        "* Colab Notebook: https://colab.research.google.com/drive/1FqZepXGWDF2PvVzncqf5oB5DIaXoydPv\n",
        "\n",
        "If you enjoyed learning about this topic or are interested in modern technologies and artificial intelligence, make sure to:\n",
        "\n",
        "* Connect with me on LinkednIn: https://www.linkedin.com/in/adam-majmudar-24b596194/\n",
        "* Follow me on medium for more interesting articles in the future: https://medium.com/@mr.adam.maj"
      ]
    }
  ]
}